import shap
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import numpy as np
from sentiment import sentiment_analysis
from processing import process_data_to_list


posts = process_data_to_list("data/posts", "content")
comms = process_data_to_list("data/comments", "body")
data = posts+comms


data = data[:10]


# Load pre-trained tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

def predict_sentiment(text):
    #numpy array handling
    if isinstance(text, np.ndarray):
        text = text.tolist()
    
    # Tokenize and encode text for the model
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    # Predict using the model
    with torch.no_grad():
        outputs = model(**inputs)
    # Return softmax probabilities
    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
    return probabilities.detach().cpu().numpy()

def shap_word_importance(texts):
    # Initialize SHAP explainer using the prediction function and tokenizer
    explainer = shap.Explainer(predict_sentiment, tokenizer)
    # Calculate SHAP values
    shap_values = explainer(texts)
    
    # Initialize list to store importance scores for each text
    importance_scores = []
    
    # Iterate over SHAP values for each text
    for shap_value in shap_values:
        # Extract SHAP values for each token
        # Note: Each token's SHAP value is stored in shap_value.values, where we sum over the output classes (axis=-1)
        token_importance = np.sum(shap_value.values, axis=-1).tolist()
        # Store the summed importance scores for each word
        importance_scores.append(token_importance)
    
    return importance_scores


# Process each text for sentiment analysis:
for text in data:
    sentiment = predict_sentiment(text)
    print(sentiment)  # This will print the sentiment probabilities for each text.

# Calculate SHAP word importance for the texts
importance_scores = shap_word_importance(data)
for i, scores in enumerate(importance_scores):
    print(f"Text {i+1} word importance scores: {scores}")


len(importance_scores)


sentiments = sentiment_analysis(data)


sentiments


def combine_sentiment_and_importance(sentiments, importance_scores):
    combined_scores = []

    for sentiment, word_scores in zip(sentiments, importance_scores):
        adjusted_scores = [sentiment * score for score in word_scores]
        combined_scores.append(adjusted_scores)

    return combined_scores


final = combine_sentiment_and_importance(sentiments, importance_scores)


final
